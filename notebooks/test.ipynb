{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq, TrainerCallback\n",
    "AUDIO_FILE_PATH = 'data/reconstructed_audio.wav'\n",
    "from utils import reconstruct_codec, flat_codec\n",
    "\n",
    "BASE_MODEL = \"BEE-spoke-data/smol_llama-101M-GQA\"\n",
    "model_path = './results/2024-05-24_22:15-fixed_seperator_test_with_less_databased_on_BEE-spoke-data/smol_llama-101M-GQA/checkpoint-30000'\n",
    "MAX_LEN = 512\n",
    "\n",
    "# load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from snac import SNAC\n",
    "snac = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\").eval()\n",
    "snac = snac.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.display import Audio\n",
    "\n",
    "dataset = load_dataset(\"blanchon/snac_llm_parler_tts\", split='train[0:100]' ) # to only get a subset for testing\n",
    "dataset = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "sample = dataset[\"test\"][0]\n",
    "tokens = [int(t) for t in sample[\"snac24khz\"].split(\" \")]\n",
    "codes = reconstruct_codec(tokens)\n",
    "audio_hat = snac.decode(codes) \n",
    "sf.write(AUDIO_FILE_PATH, audio_hat.cpu().detach().numpy().squeeze(), 24000)\n",
    "print(sample[\"text\"])\n",
    "Audio(AUDIO_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without tensors\n",
    "SEP = tokenizer(\"[audio]\")[\"input_ids\"][1:]\n",
    "\n",
    "\n",
    "input_ids = tokenizer(sample[\"text\"],  padding=False, truncation=True, max_length=512-len(SEP))[\"input_ids\"]+SEP\n",
    "#input_ids += [int(t) for t in sample[\"snac24khz\"].split(\" \")][:7] # try to enforce right voice\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(torch.tensor([input_ids]).to(\"cuda\"), max_length=MAX_LEN, pad_token_id=tokenizer.eos_token_id, temperature=0)\n",
    "    outputs = outputs[0][len(input_ids):]\n",
    "\n",
    "\n",
    "codes = reconstruct_codec(outputs)\n",
    "audio_hat = snac.decode(codes) \n",
    "sf.write(AUDIO_FILE_PATH, audio_hat.cpu().detach().numpy().squeeze(), 24000)\n",
    "print(sample[\"text\"])\n",
    "Audio(AUDIO_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default-6IQEWI-N-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
